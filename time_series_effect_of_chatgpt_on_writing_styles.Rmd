---
title: "Time Series Analysis; Effect of ChatGPT on Writing Styles"
output: html_notebook
---

```{r}


# Load required libraries
library(tidyverse)
library(nlme)        # For GLS models
library(broom)       # For model summaries
library(ggplot2)     # Visualization

# =====================
# Step 1: Load the Data
# =====================

## go over all the data sources and for each, read the file "data_sources/data_sources_prefixes_pca.csv"

# Function to fit GLS for a given column
fit_gls <- function(column_name) {
  formula <- as.formula(paste(column_name, "~ time + ONSET + POST"))
  
  # Fit GLS model with AR(1) correlation structure
  model <- gls(
    formula, 
    data = data, 
    correlation = corAR1(form = ~ as.numeric(time))
  )
  
  return(model)
}


extract_gls_results <- function(model, column_name) {
  summary_model <- summary(model)
  coefficients <- summary_model$tTable
  
  # Convert coefficients to a data frame
  results_df <- as.data.frame(coefficients) %>%
    rownames_to_column(var = "term") %>%
    mutate(
      PC = column_name,
      estimate = Value,
      std.error = Std.Error,
      statistic = `t-value`,
      p.value = `p-value`
    ) %>%
    select(PC, term, estimate, std.error, statistic, p.value)
  
  return(results_df)
}


# ==========================
# Step 5: Assumption Checks
# ==========================
# Function to check residual autocorrelation
check_residuals <- function(model, column_name) {
  residuals <- residuals(model, type = "normalized")
  
  # Perform Durbin-Watson Test
  dw_test <- nlme::corAR1(form = ~ as.numeric(time))
  
  cat("### Assumption Checks for:", column_name, "###\n")
  print(summary(dw_test))
}

visualize_gls <- function(column_name, model) {
  ggplot(data, aes(x = time, y = .data[[column_name]])) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", color = "blue") +
    labs(
      title = paste("GLS Model for", column_name),
      x = "Time",
      y = column_name
    ) +
    theme_minimal()
}


column_prefixes = c("^PC", "^similarity_", "^PC", "^mean_")
file_postfixes = c("_pca_similarity.csv", "_clean_similarities.csv", "_pca_mean.csv", "_clean_similarities.csv")
for (fix_index in 1:length(column_prefixes)) {
  column_prefix = column_prefixes[fix_index]
  file_postfix = file_postfixes[fix_index]
  data_sources = c("papers", "reddit", "news")
  data_sources_prefixes = c("cl_cv_papers", "reddit", "news")
  for (data_source_index in 1:length(data_sources)) {
      
    
    
      data_source = data_sources[data_source_index]
      data_source_prefix = data_sources_prefixes[data_source_index]
      
      print(column_prefix)
      print(file_postfix)
      print(data_source)
      print(data_source_prefix)
      
      data = read_csv(paste("data", "/", data_source, "/", data_source_prefix, file_postfix, sep=""))
      
      data <- data %>%
        mutate(
          time = as.Date(paste(year, month, "01", sep = "-")),
          ONSET = ifelse(time >= as.Date("2022-11-01"), 1, 0),
          POST = ifelse(ONSET == 1, as.numeric(time - as.Date("2022-11-01")), 0)
        )
      # Select only PC columns
      pc_columns <- names(data)[grepl(column_prefix, names(data))]
      
      # if the process is for the papers, exclude the column similarity_lex_functional_word_whoever
      if (data_source == "papers") {
        pc_columns <- pc_columns[!grepl("similarity_lex_functional_word_whoever", pc_columns)]
        pc_columns <- pc_columns[!grepl("mean_lex_functional_word_whoever", pc_columns)]
      }
      
      gls_models <- map(pc_columns, fit_gls)
      
      gls_results <- map_df(
        seq_along(pc_columns),
        ~extract_gls_results(gls_models[[.x]], pc_columns[.x])
      )
      
      
      # Filter for significant results
      significant_gls_results <- gls_results %>%
        filter(p.value < 0.05)
      
      
      # ==========================
      
      
      # Run assumption checks for significant models
      significant_pcs <- unique(significant_gls_results$PC)
      significant_gls_models <- gls_models[pc_columns %in% significant_pcs]
      
      map2(
        significant_gls_models,
        significant_pcs,
        ~check_residuals(.x, .y)
      )
      
      # Visualize significant results
      for (pc in significant_pcs) {
        model <- gls_models[[which(pc_columns == pc)]]
        print(visualize_gls(pc, model))
      }
      
      # ===========================
      # Step 7: Export Results
      # ===========================
      write_csv(significant_gls_results, paste("data", "/", data_source, "/", "significant_results_", column_prefix, data_source_prefix, file_postfix, sep=""))
    
    
  }
}

```


```{r}


# Load required libraries
library(tidyverse)
library(nlme)        # For GLS models
library(broom)       # For model summaries
library(ggplot2)     # Visualization

# =====================
# Step 1: Load the Data
# =====================

## go over all the data sources and for each, read the file "data_sources/data_sources_prefixes_pca.csv"

# Function to fit GLS for a given column
fit_gls <- function(column_name) {
  formula <- as.formula(paste(column_name, "~ time + ONSET + POST"))
  
  # Fit GLS model with AR(1) correlation structure
  model <- gls(
    formula, 
    data = data, 
    correlation = corAR1(form = ~ as.numeric(time))
  )
  
  return(model)
}


extract_gls_results <- function(model, column_name) {
  summary_model <- summary(model)
  coefficients <- summary_model$tTable
  
  # Convert coefficients to a data frame
  results_df <- as.data.frame(coefficients) %>%
    rownames_to_column(var = "term") %>%
    mutate(
      PC = column_name,
      estimate = Value,
      std.error = Std.Error,
      statistic = `t-value`,
      p.value = `p-value`
    ) %>%
    select(PC, term, estimate, std.error, statistic, p.value)
  
  return(results_df)
}


# ==========================
# Step 5: Assumption Checks
# ==========================
# Function to check residual autocorrelation
check_residuals <- function(model, column_name) {
  residuals <- residuals(model, type = "normalized")
  
  # Perform Durbin-Watson Test
  dw_test <- nlme::corAR1(form = ~ as.numeric(time))
  
  cat("### Assumption Checks for:", column_name, "###\n")
  print(summary(dw_test))
}

visualize_gls <- function(column_name, model) {
  ggplot(data, aes(x = time, y = .data[[column_name]])) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", color = "blue") +
    labs(
      title = paste("GLS Model for", column_name),
      x = "Time",
      y = column_name
    ) +
    theme_minimal()
}


file_postfixes = c("1_gram", "2_gram", "3_gram")
for (fix_index in 1:length(file_postfixes)) {
  file_postfix = file_postfixes[fix_index]
  data_sources = c("papers", "reddit", "news")
  for (data_source_index in 1:length(data_sources)) {
    
      data_source = data_sources[data_source_index]
      
      print(file_postfix)
      print(data_source)
      
      data = read_csv(paste("data", "/", data_source, "/", "monthly_similarities_mean_", file_postfix, ".csv", sep=""))
      
      data <- data %>%
        mutate(
          time = as.Date(paste(year, month, "01", sep = "-")),
          ONSET = ifelse(time >= as.Date("2022-11-01"), 1, 0),
          POST = ifelse(ONSET == 1, as.numeric(time - as.Date("2022-11-01")), 0)
        )
      # Select only PC columns
      pc_columns <- c("similarity")
      
      gls_models <- map(pc_columns, fit_gls)
      
      gls_results <- map_df(
        seq_along(pc_columns),
        ~extract_gls_results(gls_models[[.x]], pc_columns[.x])
      )
      
      
      # Filter for significant results
      significant_gls_results <- gls_results %>%
        filter(p.value < 0.05)
      
      
      # ==========================
      
      
      # Run assumption checks for significant models
      significant_pcs <- unique(significant_gls_results$PC)
      significant_gls_models <- gls_models[pc_columns %in% significant_pcs]
      
      map2(
        significant_gls_models,
        significant_pcs,
        ~check_residuals(.x, .y)
      )
      
      # Visualize significant results
      for (pc in significant_pcs) {
        model <- gls_models[[which(pc_columns == pc)]]
        print(visualize_gls(pc, model))
      }
      
      # ===========================
      # Step 7: Export Results
      # ===========================
      write_csv(significant_gls_results, paste("data", "/", data_source, "/", "significant_results_", file_postfix, ".csv", sep=""))

  }
}

```
