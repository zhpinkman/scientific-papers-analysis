{"2007_5": {"texts": ["  This paper presents a corpus-based approach to word sense disambiguation that\nbuilds an ensemble of Naive Bayesian classifiers, each of which is based on\nlexical features that represent co--occurring words in varying sized windows of\ncontext. Despite the simplicity of this approach, empirical results\ndisambiguating the widely studied nouns line and interest show that such an\nensemble achieves accuracy rivaling the best previously published results.\n", "  The NWO Priority Programme Language and Speech Technology is a 5-year\nresearch programme aiming at the development of spoken language information\nsystems. In the Programme, two alternative natural language processing (NLP)\nmodules are developed in parallel: a grammar-based (conventional, rule-based)\nmodule and a data-oriented (memory-based, stochastic, DOP) module. In order to\ncompare the NLP modules, a formal evaluation has been carried out three years\nafter the start of the Programme. This paper describes the evaluation procedure\nand the evaluation results. The grammar-based component performs much better\nthan the data-oriented one in this comparison.\n", "  Coping with ambiguity has recently received a lot of attention in natural\nlanguage processing. Most work focuses on the semantic representation of\nambiguous expressions. In this paper we complement this work in two ways.\nFirst, we provide an entailment relation for a language with ambiguous\nexpressions. Second, we give a sound and complete tableaux calculus for\nreasoning with statements involving ambiguous quantification. The calculus\ninterleaves partial disambiguation steps with steps in a traditional deductive\nprocess, so as to minimize and postpone branching in the proof process, and\nthereby increases its efficiency.\n", "  The common approach to radial distortion is by the means of polynomial\napproximation, which introduces distortion-specific parameters into the camera\nmodel and requires estimation of these distortion parameters. The task of\nestimating radial distortion is to find a radial distortion model that allows\neasy undistortion as well as satisfactory accuracy. This paper presents a new\npiecewise radial distortion model with easy analytical undistortion formula.\nThe motivation for seeking a piecewise radial distortion model is that, when a\ncamera is resulted in a low quality during manufacturing, the nonlinear radial\ndistortion can be complex. Using low order polynomials to approximate the\nradial distortion might not be precise enough. On the other hand, higher order\npolynomials suffer from the inverse problem. With the new piecewise radial\ndistortion function, more flexibility is obtained and the radial undistortion\ncan be performed analytically. Experimental results are presented to show that\nwith this new piecewise radial distortion model, better performance is achieved\nthan that using the single function. Furthermore, a comparable performance with\nthe conventional polynomial model using 2 coefficients can also be\naccomplished.\n", "  This paper examines efficient predictive broad-coverage parsing without\ndynamic programming. In contrast to bottom-up methods, depth-first top-down\nparsing produces partial parses that are fully connected trees spanning the\nentire left context, from which any kind of non-local dependency or partial\nsemantic interpretation can in principle be read. We contrast two predictive\nparsing approaches, top-down and left-corner parsing, and find both to be\nviable. In addition, we find that enhancement with non-local information not\nonly improves parser accuracy, but also substantially improves the search\nefficiency.\n", "  In this thesis, I address the problem of automatically acquiring lexical\nsemantic knowledge, especially that of case frame patterns, from large corpus\ndata and using the acquired knowledge in structural disambiguation. The\napproach I adopt has the following characteristics: (1) dividing the problem\ninto three subproblems: case slot generalization, case dependency learning, and\nword clustering (thesaurus construction). (2) viewing each subproblem as that\nof statistical estimation and defining probability models for each subproblem,\n(3) adopting the Minimum Description Length (MDL) principle as learning\nstrategy, (4) employing efficient learning algorithms, and (5) viewing the\ndisambiguation problem as that of statistical prediction. Major contributions\nof this thesis include: (1) formalization of the lexical knowledge acquisition\nproblem, (2) development of a number of learning methods for lexical knowledge\nacquisition, and (3) development of a high-performance disambiguation method.\n", "  In this paper we present a new tree-rewriting formalism called Link-Sharing\nTree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using\nLSTAG we define an approach towards coordination where linguistic dependency is\ndistinguished from the notion of constituency. Such an approach towards\ncoordination that explicitly distinguishes dependencies from constituency gives\na better formal understanding of its representation when compared to previous\napproaches that use tree-rewriting systems which conflate the two issues.\n", "  A glottochronologic retrognostic of language system is proposed\n", "  We present a novel, type-logical analysis of_polarity sensitivity_: how\nnegative polarity items (like \"any\" and \"ever\") or positive ones (like \"some\")\nare licensed or prohibited. It takes not just scopal relations but also linear\norder into account, using the programming-language notions of delimited\ncontinuations and evaluation order, respectively. It thus achieves greater\nempirical coverage than previous proposals.\n", "  The world is passing through a major revolution called the information\nrevolution, in which information and knowledge is becoming available to people\nin unprecedented amounts wherever and whenever they need it. Those societies\nwhich fail to take advantage of the new technology will be left behind, just\nlike in the industrial revolution.\n  The information revolution is based on two major technologies: computers and\ncommunication. These technologies have to be delivered in a COST EFFECTIVE\nmanner, and in LANGUAGES accessible to people.\n  One way to deliver them in cost effective manner is to make suitable\ntechnology choices (discussed later), and to allow people to access through\nshared resources. This could be done throuch street corner shops (for computer\nusage, e-mail etc.), schools, community centers and local library centres.\n", "  We present an LFG-DOP parser which uses fragments from LFG-annotated\nsentences to parse new sentences. Experiments with the Verbmobil and Homecentre\ncorpora show that (1) Viterbi n best search performs about 100 times faster\nthan Monte Carlo search while both achieve the same accuracy; (2) the DOP\nhypothesis which states that parse accuracy increases with increasing fragment\nsize is confirmed for LFG-DOP; (3) LFG-DOP's relative frequency estimator\nperforms worse than a discounted frequency estimator; and (4) LFG-DOP\nsignificantly outperforms Tree-DOP is evaluated on tree structures only.\n", "  The thesis presents an attempt at using the syntactic structure in natural\nlanguage for improved language models for speech recognition. The structured\nlanguage model merges techniques in automatic parsing and language modeling\nusing an original probabilistic parameterization of a shift-reduce parser. A\nmaximum likelihood reestimation procedure belonging to the class of\nexpectation-maximization algorithms is employed for training the model.\nExperiments on the Wall Street Journal, Switchboard and Broadcast News corpora\nshow improvement in both perplexity and word error rate - word lattice\nrescoring - over the standard 3-gram language model. The significance of the\nthesis lies in presenting an original approach to language modeling that uses\nthe hierarchical - syntactic - structure in natural language to improve on\ncurrent 3-gram modeling techniques for large vocabulary speech recognition.\n", "  We describe a recently developed corpus annotation scheme for evaluating\nparsers that avoids shortcomings of current methods. The scheme encodes\ngrammatical relations between heads and dependents, and has been used to mark\nup a new public-domain corpus of naturally occurring English text. We show how\nthe corpus can be used to evaluate the accuracy of a robust parser, and relate\nthe corpus to extant resources.\n", "  The paper investigates the use of richer syntactic dependencies in the\nstructured language model (SLM). We present two simple methods of enriching the\ndependencies in the syntactic parse trees used for intializing the SLM. We\nevaluate the impact of both methods on the perplexity (PPL) and\nword-error-rate(WER, N-best rescoring) performance of the SLM. We show that the\nnew model achieves an improvement in PPL and WER over the baseline results\nreported using the SLM on the UPenn Treebank and Wall Street Journal (WSJ)\ncorpora, respectively.\n", "  A simple search method for finding a blur convolved in a given image is\npresented. The method can be easily extended to a large blur. The method has\nbeen experimentally tested with a model blurred image.\n", "  This paper describes experiments carried out using a variety of\nmachine-learning methods, including the k-nearest neighborhood method that was\nused in a previous study, for the translation of tense, aspect, and modality.\nIt was found that the support-vector machine method was the most precise of all\nthe methods tested.\n", "  Transformation-based learning has been successfully employed to solve many\nnatural language processing problems. It achieves state-of-the-art performance\non many natural language processing tasks and does not overtrain easily.\nHowever, it does have a serious drawback: the training time is often\nintorelably long, especially on the large corpora which are often used in NLP.\nIn this paper, we present a novel and realistic method for speeding up the\ntraining time of a transformation-based learner without sacrificing\nperformance. The paper compares and contrasts the training time needed and\nperformance achieved by our modified learner with two other systems: a standard\ntransformation-based learner, and the ICA system \\cite{hepple00:tbl}. The\nresults of these experiments show that our system is able to achieve a\nsignificant improvement in training time while still achieving the same\nperformance as a standard transformation-based learner. This is a valuable\ncontribution to systems and algorithms which utilize transformation-based\nlearning at any part of the execution.\n", "  I propose a variable-free treatment of dynamic semantics. By \"dynamic\nsemantics\" I mean analyses of donkey sentences (\"Every farmer who owns a donkey\nbeats it\") and other binding and anaphora phenomena in natural language where\nmeanings of constituents are updates to information states, for instance as\nproposed by Groenendijk and Stokhof. By \"variable-free\" I mean denotational\nsemantics in which functional combinators replace variable indices and\nassignment functions, for instance as advocated by Jacobson.\n  The new theory presented here achieves a compositional treatment of dynamic\nanaphora that does not involve assignment functions, and separates the\ncombinatorics of variable-free semantics from the particular linguistic\nphenomena it treats. Integrating variable-free semantics and dynamic semantics\ngives rise to interactions that make new empirical predictions, for example\n\"donkey weak crossover\" effects.\n", "  Grammatical relationships (GRs) form an important level of natural language\nprocessing, but different sets of GRs are useful for different purposes.\nTherefore, one may often only have time to obtain a small training corpus with\nthe desired GR annotations. To boost the performance from using such a small\ntraining corpus on a transformation rule learner, we use existing systems that\nfind related types of annotations.\n", "  High dimensional, sparsely populated data spaces have been characterized in\nterms of ultrametric topology. This implies that there are natural, not\nnecessarily unique, tree or hierarchy structures defined by the ultrametric\ntopology. In this note we study the extent of local ultrametric topology in\ntexts, with the aim of finding unique ``fingerprints'' for a text or corpus,\ndiscriminating between texts from different domains, and opening up the\npossibility of exploiting hierarchical structures in the data. We use coherent\nand meaningful collections of over 1000 texts, comprising over 1.3 million\nwords.\n", "  In this paper we analyze two question answering tasks : the TREC-8 question\nanswering task and a set of reading comprehension exams. First, we show that\nQ/A systems perform better when there are multiple answer opportunities per\nquestion. Next, we analyze common approaches to two subproblems: term overlap\nfor answer sentence identification, and answer typing for short answer\nextraction. We present general tools for analyzing the strengths and\nlimitations of techniques for these subproblems. Our results quantify the\nlimitations of both term overlap and answer typing to distinguish between\ncompeting answer candidates.\n", "  Annotation graphs and annotation servers offer infrastructure to support the\nanalysis of human language resources in the form of time-series data such as\ntext, audio and video. This paper outlines areas of common need among empirical\nlinguists and computational linguists. After reviewing examples of data and\ntools used or under development for each of several areas, it proposes a common\nframework for future tool development, data annotation and resource sharing\nbased upon annotation graphs and servers.\n", "  This paper proposes a Japanese/English cross-language information retrieval\n(CLIR) system targeting technical documents. Our system first translates a\ngiven query containing technical terms into the target language, and then\nretrieves documents relevant to the translated query. The translation of\ntechnical terms is still problematic in that technical terms are often compound\nwords, and thus new terms can be progressively created simply by combining\nexisting base words. In addition, Japanese often represents loanwords based on\nits phonogram. Consequently, existing dictionaries find it difficult to achieve\nsufficient coverage. To counter the first problem, we use a compound word\ntranslation method, which uses a bilingual dictionary for base words and\ncollocational statistics to resolve translation ambiguity. For the second\nproblem, we propose a transliteration method, which identifies phonetic\nequivalents in the target language. We also show the effectiveness of our\nsystem using a test collection for CLIR.\n", "  The anusaaraka system (a kind of machine translation system) makes text in\none Indian language accessible through another Indian language. The machine\npresents an image of the source text in a language close to the target\nlanguage. In the image, some constructions of the source language (which do not\nhave equivalents in the target language) spill over to the output. Some special\nnotation is also devised.\n  Anusaarakas have been built from five pairs of languages: Telugu,Kannada,\nMarathi, Bengali and Punjabi to Hindi. They are available for use through Email\nservers.\n  Anusaarkas follows the principle of substitutibility and reversibility of\nstrings produced. This implies preservation of information while going from a\nsource language to a target language.\n  For narrow subject areas, specialized modules can be built by putting subject\ndomain knowledge into the system, which produce good quality grammatical\noutput. However, it should be remembered, that such modules will work only in\nnarrow areas, and will sometimes go wrong. In such a situation, anusaaraka\noutput will still remain useful.\n", "  Nuclear medicine (NM) images inherently suffer from large amounts of noise\nand blur. The purpose of this research is to reduce the noise and blur while\nmaintaining image integrity for improved diagnosis. The proposed solution is to\nincrease image quality after the standard pre- and post-processing undertaken\nby a gamma camera system. Mean Field Annealing (MFA) is the image processing\ntechnique used in this research. It is a computational iterative technique that\nmakes use of the Point Spread Function (PSF) and the noise associated with the\nNM image. MFA is applied to NM images with the objective of reducing noise\nwhile not compromising edge integrity. Using a sharpening filter as a\npost-processing technique (after MFA) yields image enhancement of planar NM\nimages.\n", "  We describe the CoNLL-2001 shared task: dividing text into clauses. We give\nbackground information on the data sets, present a general overview of the\nsystems that have taken part in the shared task and briefly discuss their\nperformance.\n", "  The paper aims at emphasizing that, even relaxed, the hypothesis of\ncompositionality has to face many problems when used for interpreting natural\nlanguage texts. Rather than fixing these problems within the compositional\nframework, we believe that a more radical change is necessary, and propose\nanother approach.\n", "  This paper describes how robust parsing techniques can be fruitful applied\nfor building a query generation module which is part of a pipelined NLP\narchitecture aimed at process natural language queries in a restricted domain.\nWe want to show that semantic robustness represents a key issue in those NLP\nsystems where it is more likely to have partial and ill-formed utterances due\nto various factors (e.g. noisy environments, low quality of speech recognition\nmodules, etc...) and where it is necessary to succeed, even if partially, in\nextracting some meaningful information.\n", "  Diff is a software program that detects differences between two data sets and\nis useful in natural language processing. This paper shows several examples of\nthe application of diff. They include the detection of differences between two\ndifferent datasets, extraction of rewriting rules, merging of two different\ndatasets, and the optimal matching of two different data sets. Since diff comes\nwith any standard UNIX system, it is readily available and very easy to use.\nOur studies showed that diff is a practical tool for research into natural\nlanguage processing.\n", "  This paper presents an invariant under scaling and linear brightness change.\nThe invariant is based on differentials and therefore is a local feature.\nRotationally invariant 2-d differential Gaussian operators up to third order\nare proposed for the implementation of the invariant. The performance is\nanalyzed by simulating a camera zoom-out.\n", "  In some contexts, well-formed natural language cannot be expected as input to\ninformation or communication systems. In these contexts, the use of\ngrammar-independent input (sequences of uninflected semantic units like e.g.\nlanguage-independent icons) can be an answer to the users' needs. A semantic\nanalysis can be performed, based on lexical semantic knowledge: it is\nequivalent to a dependency analysis with no syntactic or morphological clues.\nHowever, this requires that an intelligent system should be able to interpret\nthis input with reasonable accuracy and in reasonable time. Here we propose a\nmethod allowing a purely semantic-based analysis of sequences of semantic\nunits. It uses an algorithm inspired by the idea of ``chart parsing'' known in\nNatural Language Processing, which stores intermediate parsing results in order\nto bring the calculation time down. In comparison with using declarative logic\nprogramming - where the calculation time, left to a prolog engine, is\nhyperexponential -, this method brings the calculation time down to a\npolynomial time, where the order depends on the valency of the predicates.\n", "  This paper compares the tasks of part-of-speech (POS) tagging and\nword-sense-tagging or disambiguation (WSD), and argues that the tasks are not\nrelated by fineness of grain or anything like that, but are quite different\nkinds of task, particularly becuase there is nothing in POS corresponding to\nsense novelty. The paper also argues for the reintegration of sub-tasks that\nare being separated for evaluation\n", "  Log-linear models provide a statistically sound framework for Stochastic\n``Unification-Based'' Grammars (SUBGs) and stochastic versions of other kinds\nof grammars. We describe two computationally-tractable ways of estimating the\nparameters of such grammars from a training corpus of syntactic analyses, and\napply these to estimate a stochastic version of Lexical-Functional Grammar.\n", "  Cross-language information retrieval (CLIR), where queries and documents are\nin different languages, needs a translation of queries and/or documents, so as\nto standardize both of them into a common representation. For this purpose, the\nuse of machine translation is an effective approach. However, computational\ncost is prohibitive in translating large-scale document collections. To resolve\nthis problem, we propose a two-stage CLIR method. First, we translate a given\nquery into the document language, and retrieve a limited number of foreign\ndocuments. Second, we machine translate only those documents into the user\nlanguage, and re-rank them based on the translation result. We also show the\neffectiveness of our method by way of experiments using Japanese queries and\nEnglish technical documents.\n", "  We outline how utterances in dialogs can be interpreted using a partial first\norder logic. We exploit the capability of this logic to talk about the truth\nstatus of formulae to define a notion of coherence between utterances and\nexplain how this coherence relation can serve for the construction of AND/OR\ntrees that represent the segmentation of the dialog. In a BDI model we\nformalize basic assumptions about dialog and cooperative behaviour of\nparticipants. These assumptions provide a basis for inferring speech acts from\ncoherence relations between utterances and attitudes of dialog participants.\nSpeech acts prove to be useful for determining dialog segments defined on the\nnotion of completing expectations of dialog participants. Finally, we sketch\nhow explicit segmentation signalled by cue phrases and performatives is covered\nby our dialog model.\n", "  We report on two corpora to be used in the evaluation of component systems\nfor the tasks of (1) linear segmentation of text and (2) summary-directed\nsentence extraction. We present characteristics of the corpora, methods used in\nthe collection of user judgments, and an overview of the application of the\ncorpora to evaluating the component system. Finally, we discuss the problems\nand issues with construction of the test set which apply broadly to the\nconstruction of evaluation resources for language technologies.\n", "  We report about the current state of development of a document suite and its\napplications. This collection of tools for the flexible and robust processing\nof documents in German is based on the use of XML as unifying formalism for\nencoding input and output data as well as process information. It is organized\nin modules with limited responsibilities that can easily be combined into\npipelines to solve complex tasks. Strong emphasis is laid on a number of\ntechniques to deal with lexical and conceptual gaps that are typical when\nstarting a new application.\n", "  Feature extraction and matching are among central problems of computer\nvision. It is inefficent to search features over all locations and scales.\nNeurophysiological evidence shows that to locate objects in a digital image the\nhuman visual system employs visual attention to a specific object while\nignoring others. The brain also has a mechanism to search from coarse to fine.\nIn this paper, we present a feature extractor and an associated hierarchical\nsearching model to simulate such processes. With the hierarchical\nrepresentation of the object, coarse scanning is done through the matching of\nthe larger scale and precise localization is conducted through the matching of\nthe smaller scale. Experimental results justify the proposed model in its\neffectiveness and efficiency to localize features.\n", "  A model of rank polysemantic distribution with a minimal number of fitting\nparameters is offered. In an ideal case a parameter-free description of the\ndependence on the basis of one or several immediate features of the\ndistribution is possible.\n", "  The paper presents a study on the portability of statistical syntactic\nknowledge in the framework of the structured language model (SLM). We\ninvestigate the impact of porting SLM statistics from the Wall Street Journal\n(WSJ) to the Air Travel Information System (ATIS) domain. We compare this\napproach to applying the Microsoft rule-based parser (NLPwin) for the ATIS data\nand to using a small amount of data manually parsed at UPenn for gathering the\nintial SLM statistics. Surprisingly, despite the fact that it performs modestly\nin perplexity (PPL), the model initialized on WSJ parses outperforms the other\ninitialization methods based on in-domain annotated data, achieving a\nsignificant 0.4% absolute and 7% relative reduction in word error rate (WER)\nover a baseline system whose word error rate is 5.8%; the improvement measured\nrelative to the minimum WER achievable on the N-best lists we worked with is\n12%.\n", "  Bagging and boosting, two effective machine learning techniques, are applied\nto natural language parsing. Experiments using these techniques with a\ntrainable statistical parser are described. The best resulting system provides\nroughly as large of a gain in F-measure as doubling the corpus size. Error\nanalysis of the result of the boosting technique reveals some inconsistent\nannotations in the Penn Treebank, suggesting a semi-automatic method for\nfinding inconsistent treebank annotations.\n", "  In this paper, we describe a new method for constructing minimal,\ndeterministic, acyclic finite-state automata from a set of strings. Traditional\nmethods consist of two phases: the first to construct a trie, the second one to\nminimize it. Our approach is to construct a minimal automaton in a single phase\nby adding new strings one by one and minimizing the resulting automaton\non-the-fly. We present a general algorithm as well as a specialization that\nrelies upon the lexicographical ordering of the input strings.\n", "  Robert French has argued that a disembodied computer is incapable of passing\na Turing Test that includes subcognitive questions. Subcognitive questions are\ndesigned to probe the network of cultural and perceptual associations that\nhumans naturally develop as we live, embodied and embedded in the world. In\nthis paper, I show how it is possible for a disembodied computer to answer\nsubcognitive questions appropriately, contrary to French's claim. My approach\nto answering subcognitive questions is to use statistical information extracted\nfrom a very large collection of text. In particular, I show how it is possible\nto answer a sample of subcognitive questions taken from French, by issuing\nqueries to a search engine that indexes about 350 million Web pages. This\nsimple algorithm may shed light on the nature of human (sub-) cognition, but\nthe scope of this paper is limited to demonstrating that French is mistaken: a\ndisembodied computer can answer subcognitive questions.\n", "  A brief, general-audience overview of the history of natural language\nprocessing, focusing on data-driven approaches.Topics include \"Ambiguity and\nlanguage analysis\", \"Firth things first\", \"A 'C' change\", and \"The empiricists\nstrike back\".\n", "  The task of creating indicative summaries that help a searcher decide whether\nto read a particular document is a difficult task. This paper examines the\nindicative summarization task from a generation perspective, by first analyzing\nits required content via published guidelines and corpus analysis. We show how\nthese summaries can be factored into a set of document features, and how an\nimplemented content planner uses the topicality document feature to create\nindicative multidocument query-based summaries.\n", "  We describe the CoNLL-2000 shared task: dividing text into syntactically\nrelated non-overlapping groups of words, so-called text chunking. We give\nbackground information on the data sets, present a general overview of the\nsystems that have taken part in the shared task and briefly discuss their\nperformance.\n", "  Irregular pyramids are made of a stack of successively reduced graphs\nembedded in the plane. Such pyramids are used within the segmentation framework\nto encode a hierarchy of partitions. The different graph models used within the\nirregular pyramid framework encode different types of relationships between\nregions. This paper compares different graph models used within the irregular\npyramid framework according to a set of relationships between regions. We also\ndefine a new algorithm based on a pyramid of combinatorial maps which allows to\ndetermine if one region contains the other using only local calculus.\n", "  This paper discusses the challenges that arise when large speech corpora\nreceive an ever-broadening range of diverse and distinct annotations. Two case\nstudies of this process are presented: the Switchboard Corpus of telephone\nconversations and the TDT2 corpus of broadcast news. Switchboard has undergone\ntwo independent transcriptions and various types of additional annotation, all\ncarried out as separate projects that were dispersed both geographically and\nchronologically. The TDT2 corpus has also received a variety of annotations,\nbut all directly created or managed by a core group. In both cases, issues\narise involving the propagation of repairs, consistency of references, and the\nability to integrate annotations having different formats and levels of detail.\nWe describe a general framework whereby these issues can be addressed\nsuccessfully.\n", "  This paper compares two different ways of estimating statistical language\nmodels. Many statistical NLP tagging and parsing models are estimated by\nmaximizing the (joint) likelihood of the fully-observed training data. However,\nsince these applications only require the conditional probability\ndistributions, these distributions can in principle be learnt by maximizing the\nconditional likelihood of the training data. Perhaps somewhat surprisingly,\nmodels estimated by maximizing the joint were superior to models estimated by\nmaximizing the conditional, even though some of the latter models intuitively\nhad access to ``more information''.\n", "  This paper explores the automatic construction of a multilingual Lexical\nKnowledge Base from pre-existing lexical resources. We present a new and robust\napproach for linking already existing lexical/semantic hierarchies. We used a\nconstraint satisfaction algorithm (relaxation labeling) to select --among all\nthe candidate translations proposed by a bilingual dictionary-- the right\nEnglish WordNet synset for each sense in a taxonomy automatically derived from\na Spanish monolingual dictionary. Although on average, there are 15 possible\nWordNet connections for each sense in the taxonomy, the method achieves an\naccuracy over 80%. Finally, we also propose several ways in which this\ntechnique could be applied to enrich and improve existing lexical databases.\n"], "clean_texts": ["this paper presents a corpus based approach to word sense disambiguation that builds an ensemble of naive bayesian classifiers each of which is based on lexical features that represent co occurring words in varying sized windows of context despite the simplicity of this approach empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results", "the nwo priority programme language and speech technology is a 5 year research programme aiming at the development of spoken language information systems in the programme two alternative natural language processing nlp modules are developed in parallel a grammar based conventional rule based module and a data oriented memory based stochastic dop module in order to compare the nlp modules a formal evaluation has been carried out three years after the start of the programme this paper describes the evaluation procedure and the evaluation results the grammar based component performs much better than the data oriented one in this comparison", "coping with ambiguity has recently received a lot of attention in natural language processing most work focuses on the semantic representation of ambiguous expressions in this paper we complement this work in two ways first we provide an entailment relation for a language with ambiguous expressions second we give a sound and complete tableaux calculus for reasoning with statements involving ambiguous quantification the calculus interleaves partial disambiguation steps with steps in a traditional deductive process so as to minimize and postpone branching in the proof process and thereby increases its efficiency", "the common approach to radial distortion is by the means of polynomial approximation which introduces distortion specific parameters into the camera model and requires estimation of these distortion parameters the task of estimating radial distortion is to find a radial distortion model that allows easy undistortion as well as satisfactory accuracy this paper presents a new piecewise radial distortion model with easy analytical undistortion formula the motivation for seeking a piecewise radial distortion model is that when a camera is resulted in a low quality during manufacturing the nonlinear radial distortion can be complex using low order polynomials to approximate the radial distortion might not be precise enough on the other hand higher order polynomials suffer from the inverse problem with the new piecewise radial distortion function more flexibility is obtained and the radial undistortion can be performed analytically experimental results are presented to show that with this new piecewise radial distortion model better performance is achieved than that using the single function furthermore a comparable performance with the conventional polynomial model using 2 coefficients can also be accomplished", "this paper examines efficient predictive broad coverage parsing without dynamic programming in contrast to bottom up methods depth first top down parsing produces partial parses that are fully connected trees spanning the entire left context from which any kind of non local dependency or partial semantic interpretation can in principle be read we contrast two predictive parsing approaches top down and left corner parsing and find both to be viable in addition we find that enhancement with non local information not only improves parser accuracy but also substantially improves the search efficiency", "in this thesis i address the problem of automatically acquiring lexical semantic knowledge especially that of case frame patterns from large corpus data and using the acquired knowledge in structural disambiguation the approach i adopt has the following characteristics 1 dividing the problem into three subproblems case slot generalization case dependency learning and word clustering thesaurus construction 2 viewing each subproblem as that of statistical estimation and defining probability models for each subproblem 3 adopting the minimum description length mdl principle as learning strategy 4 employing efficient learning algorithms and 5 viewing the disambiguation problem as that of statistical prediction major contributions of this thesis include 1 formalization of the lexical knowledge acquisition problem 2 development of a number of learning methods for lexical knowledge acquisition and 3 development of a high performance disambiguation method", "in this paper we present a new tree rewriting formalism called link sharing tree adjoining grammar lstag which is a variant of synchronous tags using lstag we define an approach towards coordination where linguistic dependency is distinguished from the notion of constituency such an approach towards coordination that explicitly distinguishes dependencies from constituency gives a better formal understanding of its representation when compared to previous approaches that use tree rewriting systems which conflate the two issues", "a glottochronologic retrognostic of language system is proposed", "we present a novel type logical analysis of polarity sensitivity how negative polarity items like any and ever or positive ones like some are licensed or prohibited it takes not just scopal relations but also linear order into account using the programming language notions of delimited continuations and evaluation order respectively it thus achieves greater empirical coverage than previous proposals", "the world is passing through a major revolution called the information revolution in which information and knowledge is becoming available to people in unprecedented amounts wherever and whenever they need it those societies which fail to take advantage of the new technology will be left behind just like in the industrial revolution the information revolution is based on two major technologies computers and communication these technologies have to be delivered in a cost effective manner and in languages accessible to people one way to deliver them in cost effective manner is to make suitable technology choices discussed later and to allow people to access through shared resources this could be done throuch street corner shops for computer usage e mail etc schools community centers and local library centres", "we present an lfg dop parser which uses fragments from lfg annotated sentences to parse new sentences experiments with the verbmobil and homecentre corpora show that 1 viterbi n best search performs about 100 times faster than monte carlo search while both achieve the same accuracy 2 the dop hypothesis which states that parse accuracy increases with increasing fragment size is confirmed for lfg dop 3 lfg dop s relative frequency estimator performs worse than a discounted frequency estimator and 4 lfg dop significantly outperforms tree dop is evaluated on tree structures only", "the thesis presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition the structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift reduce parser a maximum likelihood reestimation procedure belonging to the class of expectation maximization algorithms is employed for training the model experiments on the wall street journal switchboard and broadcast news corpora show improvement in both perplexity and word error rate word lattice rescoring over the standard 3 gram language model the significance of the thesis lies in presenting an original approach to language modeling that uses the hierarchical syntactic structure in natural language to improve on current 3 gram modeling techniques for large vocabulary speech recognition", "we describe a recently developed corpus annotation scheme for evaluating parsers that avoids shortcomings of current methods the scheme encodes grammatical relations between heads and dependents and has been used to mark up a new public domain corpus of naturally occurring english text we show how the corpus can be used to evaluate the accuracy of a robust parser and relate the corpus to extant resources", "the paper investigates the use of richer syntactic dependencies in the structured language model slm we present two simple methods of enriching the dependencies in the syntactic parse trees used for intializing the slm we evaluate the impact of both methods on the perplexity ppl and word error rate wer n best rescoring performance of the slm we show that the new model achieves an improvement in ppl and wer over the baseline results reported using the slm on the upenn treebank and wall street journal wsj corpora respectively", "a simple search method for finding a blur convolved in a given image is presented the method can be easily extended to a large blur the method has been experimentally tested with a model blurred image", "this paper describes experiments carried out using a variety of machine learning methods including the k nearest neighborhood method that was used in a previous study for the translation of tense aspect and modality it was found that the support vector machine method was the most precise of all the methods tested", "transformation based learning has been successfully employed to solve many natural language processing problems it achieves state of the art performance on many natural language processing tasks and does not overtrain easily however it does have a serious drawback the training time is often intorelably long especially on the large corpora which are often used in nlp in this paper we present a novel and realistic method for speeding up the training time of a transformation based learner without sacrificing performance the paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems a standard transformation based learner and the ica system cite hepple00 tbl the results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation based learner this is a valuable contribution to systems and algorithms which utilize transformation based learning at any part of the execution", "i propose a variable free treatment of dynamic semantics by dynamic semantics i mean analyses of donkey sentences every farmer who owns a donkey beats it and other binding and anaphora phenomena in natural language where meanings of constituents are updates to information states for instance as proposed by groenendijk and stokhof by variable free i mean denotational semantics in which functional combinators replace variable indices and assignment functions for instance as advocated by jacobson the new theory presented here achieves a compositional treatment of dynamic anaphora that does not involve assignment functions and separates the combinatorics of variable free semantics from the particular linguistic phenomena it treats integrating variable free semantics and dynamic semantics gives rise to interactions that make new empirical predictions for example donkey weak crossover effects", "grammatical relationships grs form an important level of natural language processing but different sets of grs are useful for different purposes therefore one may often only have time to obtain a small training corpus with the desired gr annotations to boost the performance from using such a small training corpus on a transformation rule learner we use existing systems that find related types of annotations", "high dimensional sparsely populated data spaces have been characterized in terms of ultrametric topology this implies that there are natural not necessarily unique tree or hierarchy structures defined by the ultrametric topology in this note we study the extent of local ultrametric topology in texts with the aim of finding unique fingerprints for a text or corpus discriminating between texts from different domains and opening up the possibility of exploiting hierarchical structures in the data we use coherent and meaningful collections of over 1000 texts comprising over 1 3 million words", "in this paper we analyze two question answering tasks the trec 8 question answering task and a set of reading comprehension exams first we show that q a systems perform better when there are multiple answer opportunities per question next we analyze common approaches to two subproblems term overlap for answer sentence identification and answer typing for short answer extraction we present general tools for analyzing the strengths and limitations of techniques for these subproblems our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates", "annotation graphs and annotation servers offer infrastructure to support the analysis of human language resources in the form of time series data such as text audio and video this paper outlines areas of common need among empirical linguists and computational linguists after reviewing examples of data and tools used or under development for each of several areas it proposes a common framework for future tool development data annotation and resource sharing based upon annotation graphs and servers", "this paper proposes a japanese english cross language information retrieval clir system targeting technical documents our system first translates a given query containing technical terms into the target language and then retrieves documents relevant to the translated query the translation of technical terms is still problematic in that technical terms are often compound words and thus new terms can be progressively created simply by combining existing base words in addition japanese often represents loanwords based on its phonogram consequently existing dictionaries find it difficult to achieve sufficient coverage to counter the first problem we use a compound word translation method which uses a bilingual dictionary for base words and collocational statistics to resolve translation ambiguity for the second problem we propose a transliteration method which identifies phonetic equivalents in the target language we also show the effectiveness of our system using a test collection for clir", "the anusaaraka system a kind of machine translation system makes text in one indian language accessible through another indian language the machine presents an image of the source text in a language close to the target language in the image some constructions of the source language which do not have equivalents in the target language spill over to the output some special notation is also devised anusaarakas have been built from five pairs of languages telugu kannada marathi bengali and punjabi to hindi they are available for use through email servers anusaarkas follows the principle of substitutibility and reversibility of strings produced this implies preservation of information while going from a source language to a target language for narrow subject areas specialized modules can be built by putting subject domain knowledge into the system which produce good quality grammatical output however it should be remembered that such modules will work only in narrow areas and will sometimes go wrong in such a situation anusaaraka output will still remain useful", "nuclear medicine nm images inherently suffer from large amounts of noise and blur the purpose of this research is to reduce the noise and blur while maintaining image integrity for improved diagnosis the proposed solution is to increase image quality after the standard pre and post processing undertaken by a gamma camera system mean field annealing mfa is the image processing technique used in this research it is a computational iterative technique that makes use of the point spread function psf and the noise associated with the nm image mfa is applied to nm images with the objective of reducing noise while not compromising edge integrity using a sharpening filter as a post processing technique after mfa yields image enhancement of planar nm images", "we describe the conll 2001 shared task dividing text into clauses we give background information on the data sets present a general overview of the systems that have taken part in the shared task and briefly discuss their performance", "the paper aims at emphasizing that even relaxed the hypothesis of compositionality has to face many problems when used for interpreting natural language texts rather than fixing these problems within the compositional framework we believe that a more radical change is necessary and propose another approach", "this paper describes how robust parsing techniques can be fruitful applied for building a query generation module which is part of a pipelined nlp architecture aimed at process natural language queries in a restricted domain we want to show that semantic robustness represents a key issue in those nlp systems where it is more likely to have partial and ill formed utterances due to various factors e g noisy environments low quality of speech recognition modules etc and where it is necessary to succeed even if partially in extracting some meaningful information", "diff is a software program that detects differences between two data sets and is useful in natural language processing this paper shows several examples of the application of diff they include the detection of differences between two different datasets extraction of rewriting rules merging of two different datasets and the optimal matching of two different data sets since diff comes with any standard unix system it is readily available and very easy to use our studies showed that diff is a practical tool for research into natural language processing", "this paper presents an invariant under scaling and linear brightness change the invariant is based on differentials and therefore is a local feature rotationally invariant 2 d differential gaussian operators up to third order are proposed for the implementation of the invariant the performance is analyzed by simulating a camera zoom out", "in some contexts well formed natural language cannot be expected as input to information or communication systems in these contexts the use of grammar independent input sequences of uninflected semantic units like e g language independent icons can be an answer to the users needs a semantic analysis can be performed based on lexical semantic knowledge it is equivalent to a dependency analysis with no syntactic or morphological clues however this requires that an intelligent system should be able to interpret this input with reasonable accuracy and in reasonable time here we propose a method allowing a purely semantic based analysis of sequences of semantic units it uses an algorithm inspired by the idea of chart parsing known in natural language processing which stores intermediate parsing results in order to bring the calculation time down in comparison with using declarative logic programming where the calculation time left to a prolog engine is hyperexponential this method brings the calculation time down to a polynomial time where the order depends on the valency of the predicates", "this paper compares the tasks of part of speech pos tagging and word sense tagging or disambiguation wsd and argues that the tasks are not related by fineness of grain or anything like that but are quite different kinds of task particularly becuase there is nothing in pos corresponding to sense novelty the paper also argues for the reintegration of sub tasks that are being separated for evaluation", "log linear models provide a statistically sound framework for stochastic unification based grammars subgs and stochastic versions of other kinds of grammars we describe two computationally tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses and apply these to estimate a stochastic version of lexical functional grammar", "cross language information retrieval clir where queries and documents are in different languages needs a translation of queries and or documents so as to standardize both of them into a common representation for this purpose the use of machine translation is an effective approach however computational cost is prohibitive in translating large scale document collections to resolve this problem we propose a two stage clir method first we translate a given query into the document language and retrieve a limited number of foreign documents second we machine translate only those documents into the user language and re rank them based on the translation result we also show the effectiveness of our method by way of experiments using japanese queries and english technical documents", "we outline how utterances in dialogs can be interpreted using a partial first order logic we exploit the capability of this logic to talk about the truth status of formulae to define a notion of coherence between utterances and explain how this coherence relation can serve for the construction of and or trees that represent the segmentation of the dialog in a bdi model we formalize basic assumptions about dialog and cooperative behaviour of participants these assumptions provide a basis for inferring speech acts from coherence relations between utterances and attitudes of dialog participants speech acts prove to be useful for determining dialog segments defined on the notion of completing expectations of dialog participants finally we sketch how explicit segmentation signalled by cue phrases and performatives is covered by our dialog model", "we report on two corpora to be used in the evaluation of component systems for the tasks of 1 linear segmentation of text and 2 summary directed sentence extraction we present characteristics of the corpora methods used in the collection of user judgments and an overview of the application of the corpora to evaluating the component system finally we discuss the problems and issues with construction of the test set which apply broadly to the construction of evaluation resources for language technologies", "we report about the current state of development of a document suite and its applications this collection of tools for the flexible and robust processing of documents in german is based on the use of xml as unifying formalism for encoding input and output data as well as process information it is organized in modules with limited responsibilities that can easily be combined into pipelines to solve complex tasks strong emphasis is laid on a number of techniques to deal with lexical and conceptual gaps that are typical when starting a new application", "feature extraction and matching are among central problems of computer vision it is inefficent to search features over all locations and scales neurophysiological evidence shows that to locate objects in a digital image the human visual system employs visual attention to a specific object while ignoring others the brain also has a mechanism to search from coarse to fine in this paper we present a feature extractor and an associated hierarchical searching model to simulate such processes with the hierarchical representation of the object coarse scanning is done through the matching of the larger scale and precise localization is conducted through the matching of the smaller scale experimental results justify the proposed model in its effectiveness and efficiency to localize features", "a model of rank polysemantic distribution with a minimal number of fitting parameters is offered in an ideal case a parameter free description of the dependence on the basis of one or several immediate features of the distribution is possible", "the paper presents a study on the portability of statistical syntactic knowledge in the framework of the structured language model slm we investigate the impact of porting slm statistics from the wall street journal wsj to the air travel information system atis domain we compare this approach to applying the microsoft rule based parser nlpwin for the atis data and to using a small amount of data manually parsed at upenn for gathering the intial slm statistics surprisingly despite the fact that it performs modestly in perplexity ppl the model initialized on wsj parses outperforms the other initialization methods based on in domain annotated data achieving a significant 0 4 absolute and 7 relative reduction in word error rate wer over a baseline system whose word error rate is 5 8 the improvement measured relative to the minimum wer achievable on the n best lists we worked with is 12", "bagging and boosting two effective machine learning techniques are applied to natural language parsing experiments using these techniques with a trainable statistical parser are described the best resulting system provides roughly as large of a gain in f measure as doubling the corpus size error analysis of the result of the boosting technique reveals some inconsistent annotations in the penn treebank suggesting a semi automatic method for finding inconsistent treebank annotations", "in this paper we describe a new method for constructing minimal deterministic acyclic finite state automata from a set of strings traditional methods consist of two phases the first to construct a trie the second one to minimize it our approach is to construct a minimal automaton in a single phase by adding new strings one by one and minimizing the resulting automaton on the fly we present a general algorithm as well as a specialization that relies upon the lexicographical ordering of the input strings", "robert french has argued that a disembodied computer is incapable of passing a turing test that includes subcognitive questions subcognitive questions are designed to probe the network of cultural and perceptual associations that humans naturally develop as we live embodied and embedded in the world in this paper i show how it is possible for a disembodied computer to answer subcognitive questions appropriately contrary to french s claim my approach to answering subcognitive questions is to use statistical information extracted from a very large collection of text in particular i show how it is possible to answer a sample of subcognitive questions taken from french by issuing queries to a search engine that indexes about 350 million web pages this simple algorithm may shed light on the nature of human sub cognition but the scope of this paper is limited to demonstrating that french is mistaken a disembodied computer can answer subcognitive questions", "a brief general audience overview of the history of natural language processing focusing on data driven approaches topics include ambiguity and language analysis firth things first a c change and the empiricists strike back", "the task of creating indicative summaries that help a searcher decide whether to read a particular document is a difficult task this paper examines the indicative summarization task from a generation perspective by first analyzing its required content via published guidelines and corpus analysis we show how these summaries can be factored into a set of document features and how an implemented content planner uses the topicality document feature to create indicative multidocument query based summaries", "we describe the conll 2000 shared task dividing text into syntactically related non overlapping groups of words so called text chunking we give background information on the data sets present a general overview of the systems that have taken part in the shared task and briefly discuss their performance", "irregular pyramids are made of a stack of successively reduced graphs embedded in the plane such pyramids are used within the segmentation framework to encode a hierarchy of partitions the different graph models used within the irregular pyramid framework encode different types of relationships between regions this paper compares different graph models used within the irregular pyramid framework according to a set of relationships between regions we also define a new algorithm based on a pyramid of combinatorial maps which allows to determine if one region contains the other using only local calculus", "this paper discusses the challenges that arise when large speech corpora receive an ever broadening range of diverse and distinct annotations two case studies of this process are presented the switchboard corpus of telephone conversations and the tdt2 corpus of broadcast news switchboard has undergone two independent transcriptions and various types of additional annotation all carried out as separate projects that were dispersed both geographically and chronologically the tdt2 corpus has also received a variety of annotations but all directly created or managed by a core group in both cases issues arise involving the propagation of repairs consistency of references and the ability to integrate annotations having different formats and levels of detail we describe a general framework whereby these issues can be addressed successfully", "this paper compares two different ways of estimating statistical language models many statistical nlp tagging and parsing models are estimated by maximizing the joint likelihood of the fully observed training data however since these applications only require the conditional probability distributions these distributions can in principle be learnt by maximizing the conditional likelihood of the training data perhaps somewhat surprisingly models estimated by maximizing the joint were superior to models estimated by maximizing the conditional even though some of the latter models intuitively had access to more information", "this paper explores the automatic construction of a multilingual lexical knowledge base from pre existing lexical resources we present a new and robust approach for linking already existing lexical semantic hierarchies we used a constraint satisfaction algorithm relaxation labeling to select among all the candidate translations proposed by a bilingual dictionary the right english wordnet synset for each sense in a taxonomy automatically derived from a spanish monolingual dictionary although on average there are 15 possible wordnet connections for each sense in the taxonomy the method achieves an accuracy over 80 finally we also propose several ways in which this technique could be applied to enrich and improve existing lexical databases"], "similarities": {